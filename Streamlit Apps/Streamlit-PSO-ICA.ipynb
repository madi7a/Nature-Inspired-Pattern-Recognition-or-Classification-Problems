{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "vakhDJrdYH1Y",
        "outputId": "13685ae5-79b5-4c2b-e06f-6ff89c651bce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m81.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m86.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!pip install streamlit pandas matplotlib seaborn pyngrok -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok authtoken 2x31PIaY2yj30FAtc55kKHWVhM5_3dLz4UzeatnQKYfAPoodQ"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "w8gA1sqnYWn_",
        "outputId": "02506621-5ef2-43ac-a93d-d5ad795e1d17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile dashboard.py\n",
        "# This line is for Colab. If running locally, you don't need it.\n",
        "# Just save the content below as dashboard.py\n",
        "\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import ast\n",
        "import numpy as np\n",
        "\n",
        "# --- Page Configuration ---\n",
        "st.set_page_config(\n",
        "    page_title=\"Advanced SVM Tuning Dashboard\",\n",
        "    page_icon=\"ðŸš€\",\n",
        "    layout=\"wide\"\n",
        ")\n",
        "\n",
        "# --- Constants and Helper Functions ---\n",
        "CSV_FILE_PATH = '/content/drive/My Drive/all_svm_tuning_results.csv' # UPDATE IF YOUR CSV IS ELSEWHERE\n",
        "\n",
        "@st.cache_data\n",
        "def load_data(path):\n",
        "    try:\n",
        "        data = pd.read_csv(path)\n",
        "        if 'Test Accuracy' in data.columns:\n",
        "            data['Test Error Rate'] = 1 - data['Test Accuracy']\n",
        "        return data\n",
        "    except FileNotFoundError:\n",
        "        st.error(f\"Error: CSV file not found at '{path}'. Ensure it's in the correct location.\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error loading data: {e}\")\n",
        "        return None\n",
        "\n",
        "def parse_best_parameters(param_string):\n",
        "    if pd.isna(param_string) or param_string == \"\": return None\n",
        "    try:\n",
        "        evaluated = ast.literal_eval(param_string)\n",
        "        if isinstance(evaluated, list) and len(evaluated) == 2: return {'C': evaluated[0], 'gamma': evaluated[1]}\n",
        "        elif isinstance(evaluated, dict):\n",
        "            if 'svc__C' in evaluated and 'svc__gamma' in evaluated: return {'C': evaluated['svc__C'], 'gamma': evaluated['svc__gamma']}\n",
        "            return evaluated\n",
        "        return None\n",
        "    except: return None\n",
        "\n",
        "def generate_simulated_learning_curves(method_name, num_runs=30, num_iterations=10, final_mean_f1=0.67, final_std_f1=0.02):\n",
        "    all_runs_history = []\n",
        "    for _ in range(num_runs):\n",
        "        run_history = []\n",
        "        current_f1 = 0.3 + np.random.rand() * 0.1\n",
        "        for i in range(num_iterations):\n",
        "            improvement = (final_mean_f1 - current_f1) * (np.random.rand() * 0.3 + 0.1) * (1 - i/num_iterations)**0.5\n",
        "            noise = (np.random.rand() - 0.5) * 0.02\n",
        "            current_f1 += improvement + noise\n",
        "            current_f1 = np.clip(current_f1, 0, 1.0)\n",
        "            run_history.append(current_f1)\n",
        "        run_history[-1] = np.random.normal(loc=final_mean_f1, scale=final_std_f1/2)\n",
        "        run_history[-1] = np.clip(run_history[-1], 0, 1.0)\n",
        "        all_runs_history.append(run_history)\n",
        "    df_lc = pd.DataFrame(all_runs_history).T\n",
        "    lc_summary = pd.DataFrame({\n",
        "        'Iteration': range(1, num_iterations + 1),\n",
        "        'Mean_Val_F1': df_lc.mean(axis=1),\n",
        "        'Std_Val_F1': df_lc.std(axis=1)\n",
        "    })\n",
        "    lc_summary['Lower_Bound'] = lc_summary['Mean_Val_F1'] - lc_summary['Std_Val_F1']\n",
        "    lc_summary['Upper_Bound'] = lc_summary['Mean_Val_F1'] + lc_summary['Std_Val_F1']\n",
        "    return lc_summary\n",
        "\n",
        "results_df = load_data(CSV_FILE_PATH)\n",
        "\n",
        "# --- Main Application ---\n",
        "st.markdown(\"## ðŸš€ Advanced SVM Hyperparameter Tuning Dashboard\")\n",
        "\n",
        "if results_df is not None:\n",
        "    st.sidebar.header(\"Global Controls\")\n",
        "    all_methods = sorted(results_df['Method'].unique())\n",
        "    default_compare_methods = [m for m in ['Baseline', 'GridSearch', 'PSO', 'ICA'] if m in all_methods]\n",
        "    selected_methods_compare = st.sidebar.multiselect(\n",
        "        \"Select methods for comparison plots:\",\n",
        "        options=all_methods,\n",
        "        default=default_compare_methods\n",
        "    )\n",
        "\n",
        "    if not selected_methods_compare:\n",
        "        st.warning(\"Please select at least one method in the sidebar for comparison plots.\")\n",
        "        st.stop()\n",
        "    df_to_compare = results_df[results_df['Method'].isin(selected_methods_compare)]\n",
        "\n",
        "    st.markdown(\"### I. Overall Performance & Cost Comparison\")\n",
        "    st.markdown(f\"Comparing: `{'`, `'.join(selected_methods_compare)}`\")\n",
        "\n",
        "    FIG_WIDTH = 7\n",
        "    FIG_HEIGHT = 5\n",
        "    TITLE_FONTSIZE = 10\n",
        "    LABEL_FONTSIZE = 8\n",
        "\n",
        "    # --- Grid of 6 plots (2 columns, 3 rows) ---\n",
        "    # We will use bar plots to show mean values instead of box plots for the main metrics.\n",
        "    # For Tuning Time, a box plot is still good to show distribution, but without log scale checkbox.\n",
        "\n",
        "    row1_col1, row1_col2 = st.columns(2)\n",
        "    with row1_col1:\n",
        "        st.markdown(\"##### Average Test F1-Score\")\n",
        "        avg_f1_data = df_to_compare.groupby('Method')['Test F1-Score'].mean().reindex(selected_methods_compare).reset_index()\n",
        "        fig_f1, ax_f1 = plt.subplots(figsize=(FIG_WIDTH, FIG_HEIGHT))\n",
        "        sns.barplot(x='Method', y='Test F1-Score', data=avg_f1_data, ax=ax_f1, order=selected_methods_compare)\n",
        "        ax_f1.set_title(\"Average Test F1-Score\", fontsize=TITLE_FONTSIZE)\n",
        "        ax_f1.set_xlabel(\"Method\", fontsize=LABEL_FONTSIZE); ax_f1.set_ylabel(\"Average Test F1-Score\", fontsize=LABEL_FONTSIZE)\n",
        "        plt.xticks(rotation=45, ha='right', fontsize=LABEL_FONTSIZE); plt.yticks(fontsize=LABEL_FONTSIZE)\n",
        "        plt.tight_layout(); st.pyplot(fig_f1)\n",
        "    with row1_col2:\n",
        "        st.markdown(\"##### Average Test Accuracy\") # This was already a bar plot, keeping it.\n",
        "        avg_acc_data = df_to_compare.groupby('Method')['Test Accuracy'].mean().reindex(selected_methods_compare).reset_index()\n",
        "        fig_acc, ax_acc = plt.subplots(figsize=(FIG_WIDTH, FIG_HEIGHT))\n",
        "        sns.barplot(x='Method', y='Test Accuracy', data=avg_acc_data, ax=ax_acc, order=selected_methods_compare)\n",
        "        ax_acc.set_title(\"Average Test Accuracy\", fontsize=TITLE_FONTSIZE)\n",
        "        ax_acc.set_xlabel(\"Method\", fontsize=LABEL_FONTSIZE); ax_acc.set_ylabel(\"Average Test Accuracy\", fontsize=LABEL_FONTSIZE)\n",
        "        plt.xticks(rotation=45, ha='right', fontsize=LABEL_FONTSIZE); plt.yticks(fontsize=LABEL_FONTSIZE)\n",
        "        plt.tight_layout(); st.pyplot(fig_acc)\n",
        "\n",
        "    row2_col1, row2_col2 = st.columns(2)\n",
        "    with row2_col1:\n",
        "        st.markdown(\"##### Average Test ROC AUC\")\n",
        "        avg_roc_data = df_to_compare.groupby('Method')['Test ROC AUC'].mean().reindex(selected_methods_compare).reset_index()\n",
        "        fig_roc, ax_roc = plt.subplots(figsize=(FIG_WIDTH, FIG_HEIGHT))\n",
        "        sns.barplot(x='Method', y='Test ROC AUC', data=avg_roc_data, ax=ax_roc, order=selected_methods_compare)\n",
        "        ax_roc.set_title(\"Average Test ROC AUC\", fontsize=TITLE_FONTSIZE)\n",
        "        ax_roc.set_xlabel(\"Method\", fontsize=LABEL_FONTSIZE); ax_roc.set_ylabel(\"Average Test ROC AUC\", fontsize=LABEL_FONTSIZE)\n",
        "        plt.xticks(rotation=45, ha='right', fontsize=LABEL_FONTSIZE); plt.yticks(fontsize=LABEL_FONTSIZE)\n",
        "        plt.tight_layout(); st.pyplot(fig_roc)\n",
        "    with row2_col2:\n",
        "        st.markdown(\"##### Tuning Time (s) Distribution\") # Box plot is good here to show variability\n",
        "        fig_time, ax_time = plt.subplots(figsize=(FIG_WIDTH, FIG_HEIGHT))\n",
        "        sns.boxplot(x='Method', y='Tuning Time (s)', data=df_to_compare, ax=ax_time, order=selected_methods_compare)\n",
        "        ax_time.set_yscale('log') # Defaulting to log scale for tuning time as it often varies a lot\n",
        "        ax_time.set_title(\"Distribution of Tuning Time (s) (Log Scale)\", fontsize=TITLE_FONTSIZE)\n",
        "        ax_time.set_xlabel(\"Method\", fontsize=LABEL_FONTSIZE); ax_time.set_ylabel(\"Tuning Time (s)\", fontsize=LABEL_FONTSIZE)\n",
        "        plt.xticks(rotation=45, ha='right', fontsize=LABEL_FONTSIZE); plt.yticks(fontsize=LABEL_FONTSIZE)\n",
        "        plt.tight_layout(); st.pyplot(fig_time)\n",
        "\n",
        "    # The \"Avg. Test Accuracy Comparison\" and \"Avg. Tuning Time Comparison\" from previous row 2 are now redundant\n",
        "    # as we've made the primary plots bar plots of means.\n",
        "    # We can use this space for other useful comparisons or keep it cleaner.\n",
        "    # For now, let's remove the 3rd row of this section to simplify.\n",
        "\n",
        "    # --- Section 2: CI Algorithm Learning Curves (Simulated) ---\n",
        "    st.markdown(\"### II. CI Algorithm Learning Curves (Simulated Data)\")\n",
        "    ci_methods_for_lc = [m for m in ['PSO', 'ICA'] if m in selected_methods_compare]\n",
        "    if ci_methods_for_lc:\n",
        "        selected_lc_method = st.selectbox(\"Select CI method for Learning Curve:\", ci_methods_for_lc)\n",
        "        actual_final_f1_stats = results_df[results_df['Method'] == selected_lc_method]['Best Training Val F1-Score']\n",
        "        sim_final_mean_f1 = actual_final_f1_stats.mean() if not actual_final_f1_stats.empty else 0.67\n",
        "        sim_final_std_f1 = actual_final_f1_stats.std() if not actual_final_f1_stats.empty else 0.02\n",
        "        num_iterations_sim = 10\n",
        "        lc_data_simulated = generate_simulated_learning_curves(selected_lc_method, num_iterations=num_iterations_sim, final_mean_f1=sim_final_mean_f1, final_std_f1=sim_final_std_f1)\n",
        "\n",
        "        fig_lc, ax_lc = plt.subplots(figsize=(FIG_WIDTH*1.5, FIG_HEIGHT))\n",
        "        ax_lc.plot(lc_data_simulated['Iteration'], lc_data_simulated['Mean_Val_F1'], marker='o', label=f'Avg. Val F1 ({selected_lc_method})')\n",
        "        ax_lc.fill_between(lc_data_simulated['Iteration'], lc_data_simulated['Lower_Bound'], lc_data_simulated['Upper_Bound'], alpha=0.2, label='Std. Dev.')\n",
        "        ax_lc.set_title(f\"Simulated Learning Curve for {selected_lc_method}\", fontsize=TITLE_FONTSIZE)\n",
        "        ax_lc.set_xlabel(\"Iteration / Decade (Simulated)\", fontsize=LABEL_FONTSIZE); ax_lc.set_ylabel(\"Average Validation F1-Score (Simulated)\", fontsize=LABEL_FONTSIZE)\n",
        "        plt.xticks(fontsize=LABEL_FONTSIZE); plt.yticks(fontsize=LABEL_FONTSIZE)\n",
        "        ax_lc.legend(fontsize=LABEL_FONTSIZE); ax_lc.grid(True)\n",
        "        plt.tight_layout(); st.pyplot(fig_lc)\n",
        "    else:\n",
        "        st.info(\"Select PSO or ICA in sidebar to view (simulated) learning curves.\")\n",
        "\n",
        "    # --- Section 3: Error Rate Visualization ---\n",
        "    st.markdown(\"### III. Average Test Error Rate Comparison\")\n",
        "    if 'Test Error Rate' in df_to_compare.columns:\n",
        "        avg_err_data = df_to_compare.groupby('Method')['Test Error Rate'].mean().reindex(selected_methods_compare).reset_index()\n",
        "        fig_err, ax_err = plt.subplots(figsize=(FIG_WIDTH*1.5, FIG_HEIGHT))\n",
        "        sns.barplot(x='Method', y='Test Error Rate', data=avg_err_data, ax=ax_err, order=selected_methods_compare) # Changed to barplot\n",
        "        ax_err.set_title(\"Average Test Error Rate (1 - Accuracy) by Method\", fontsize=TITLE_FONTSIZE)\n",
        "        ax_err.set_xlabel(\"Method\", fontsize=LABEL_FONTSIZE); ax_err.set_ylabel(\"Average Test Error Rate\", fontsize=LABEL_FONTSIZE)\n",
        "        plt.xticks(rotation=45, ha='right', fontsize=LABEL_FONTSIZE); plt.yticks(fontsize=LABEL_FONTSIZE)\n",
        "        plt.tight_layout(); st.pyplot(fig_err)\n",
        "    else:\n",
        "        st.warning(\"Test Error Rate column not found. Ensure 'Test Accuracy' is in the CSV.\")\n",
        "\n",
        "    # --- Section 4: CI Algorithm Deep Dive (Parameter Distributions) ---\n",
        "    st.markdown(\"### IV. CI Algorithm Hyperparameter Search Analysis (PSO & ICA)\")\n",
        "    ci_methods_params = [m for m in ['PSO', 'ICA'] if m in selected_methods_compare]\n",
        "    if ci_methods_params:\n",
        "        selected_ci_params_method = st.selectbox(\"Select CI algorithm for parameter analysis:\", ci_methods_params, key=\"ci_param_select\")\n",
        "        df_ci_params_dive = results_df[results_df['Method'] == selected_ci_params_method]\n",
        "        if not df_ci_params_dive.empty:\n",
        "            st.markdown(f\"##### Distribution of Best Hyperparameters Found by {selected_ci_params_method}\")\n",
        "            parsed_params = df_ci_params_dive['Best Parameters'].apply(parse_best_parameters)\n",
        "            valid_params_list = [p for p in parsed_params if isinstance(p, dict) and 'C' in p and 'gamma' in p]\n",
        "            if valid_params_list:\n",
        "                df_params = pd.DataFrame(valid_params_list)\n",
        "                param_col1, param_col2 = st.columns(2)\n",
        "                with param_col1:\n",
        "                    fig_c, ax_c = plt.subplots(figsize=(FIG_WIDTH, FIG_HEIGHT)); sns.histplot(df_params['C'], kde=True, ax=ax_c, log_scale=st.checkbox(f\"Log scale for C ({selected_ci_params_method})\", True, key=f\"log_c_{selected_ci_params_method}_detail\"))\n",
        "                    ax_c.set_title(f'Distribution of Best C', fontsize=TITLE_FONTSIZE)\n",
        "                    ax_c.set_xlabel(\"C value\", fontsize=LABEL_FONTSIZE); ax_c.set_ylabel(\"Frequency\", fontsize=LABEL_FONTSIZE)\n",
        "                    plt.xticks(fontsize=LABEL_FONTSIZE); plt.yticks(fontsize=LABEL_FONTSIZE)\n",
        "                    plt.tight_layout(); st.pyplot(fig_c)\n",
        "                with param_col2:\n",
        "                    fig_g, ax_g = plt.subplots(figsize=(FIG_WIDTH, FIG_HEIGHT)); sns.histplot(df_params['gamma'], kde=True, ax=ax_g, log_scale=st.checkbox(f\"Log scale for gamma ({selected_ci_params_method})\", True, key=f\"log_g_{selected_ci_params_method}_detail\"))\n",
        "                    ax_g.set_title(f'Distribution of Best gamma', fontsize=TITLE_FONTSIZE)\n",
        "                    ax_g.set_xlabel(\"Gamma value\", fontsize=LABEL_FONTSIZE); ax_g.set_ylabel(\"Frequency\", fontsize=LABEL_FONTSIZE)\n",
        "                    plt.xticks(fontsize=LABEL_FONTSIZE); plt.yticks(fontsize=LABEL_FONTSIZE)\n",
        "                    plt.tight_layout(); st.pyplot(fig_g)\n",
        "\n",
        "                if 'Test F1-Score' in df_ci_params_dive.columns:\n",
        "                    df_params_plot = df_params.copy()\n",
        "                    f1_scores_for_plot = []\n",
        "                    original_indices = parsed_params.dropna().index\n",
        "                    for idx in original_indices:\n",
        "                         if isinstance(parsed_params.loc[idx], dict) and 'C' in parsed_params.loc[idx] and 'gamma' in parsed_params.loc[idx]:\n",
        "                            f1_scores_for_plot.append(df_ci_params_dive.loc[idx, 'Test F1-Score'])\n",
        "                    if len(f1_scores_for_plot) == len(df_params_plot): df_params_plot['Test F1-Score'] = f1_scores_for_plot\n",
        "\n",
        "                    fig_c_gamma, ax_c_gamma = plt.subplots(figsize=(FIG_WIDTH, FIG_HEIGHT))\n",
        "                    if 'Test F1-Score' in df_params_plot.columns:\n",
        "                        scatter = ax_c_gamma.scatter(df_params_plot['C'], df_params_plot['gamma'], c=df_params_plot['Test F1-Score'], cmap='viridis', alpha=0.7, s=50)\n",
        "                        cbar = plt.colorbar(scatter, ax=ax_c_gamma, label='Test F1-Score')\n",
        "                        cbar.ax.tick_params(labelsize=LABEL_FONTSIZE)\n",
        "                        cbar.set_label('Test F1-Score', size=LABEL_FONTSIZE)\n",
        "                    else: ax_c_gamma.scatter(df_params_plot['C'], df_params_plot['gamma'], alpha=0.7, s=50)\n",
        "                    ax_c_gamma.set_title(f'Best (C, gamma) pairs by {selected_ci_params_method}', fontsize=TITLE_FONTSIZE)\n",
        "                    ax_c_gamma.set_xlabel('C value', fontsize=LABEL_FONTSIZE); ax_c_gamma.set_ylabel('gamma value', fontsize=LABEL_FONTSIZE)\n",
        "                    if st.checkbox(f\"Log scales for C & gamma ({selected_ci_params_method})\", True, key=f\"log_cg_{selected_ci_params_method}_detail\"):\n",
        "                        ax_c_gamma.set_xscale('log'); ax_c_gamma.set_yscale('log')\n",
        "                    plt.xticks(fontsize=LABEL_FONTSIZE); plt.yticks(fontsize=LABEL_FONTSIZE)\n",
        "                    plt.tight_layout(); st.pyplot(fig_c_gamma)\n",
        "            else: st.warning(f\"Could not parse C/gamma for {selected_ci_params_method}.\")\n",
        "    else: st.info(\"Select PSO or ICA in sidebar to see parameter analysis.\")\n",
        "\n",
        "    st.markdown(\"### V. Detailed Run Data\")\n",
        "    with st.expander(\"Explore Specific Runs\"):\n",
        "        explore_method_detail = st.selectbox(\"Select method:\", options=all_methods, key=\"explore_method_specific\")\n",
        "        if explore_method_detail:\n",
        "            df_explore_specific = results_df[results_df['Method'] == explore_method_detail]\n",
        "            if not df_explore_specific.empty:\n",
        "                if explore_method_detail in ['PSO', 'ICA']:\n",
        "                    available_seeds_detail = sorted(df_explore_specific['Seed'].dropna().unique().astype(int))\n",
        "                    if available_seeds_detail:\n",
        "                        selected_seed_detail = st.select_slider(f\"Select Seed for {explore_method_detail}:\", options=available_seeds_detail)\n",
        "                        st.dataframe(df_explore_specific[df_explore_specific['Seed'] == selected_seed_detail])\n",
        "                    else: st.info(f\"No runs with seeds for {explore_method_detail}.\")\n",
        "                else: st.dataframe(df_explore_specific)\n",
        "            else: st.info(f\"No data for method: {explore_method_detail}\")\n",
        "    with st.expander(\"Show/Hide Full Raw Data Table\"): st.dataframe(results_df)\n",
        "    st.sidebar.markdown(\"---\");\n",
        "else:\n",
        "    st.error(\"ðŸš¨ Data could not be loaded. Please check the CSV file and its path (`CSV_FILE_PATH`).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIwtLtmJalDX",
        "outputId": "41e33dbe-cdec-48e9-88b1-3b017744fb92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting dashboard.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Run Streamlit App and Expose with ngrok (Foreground Debugging Mode with ngrok checks)\n",
        "\n",
        "from pyngrok import ngrok, conf\n",
        "import subprocess\n",
        "import os\n",
        "import time\n",
        "\n",
        "# --- Kill existing ngrok/streamlit processes (best effort) ---\n",
        "try:\n",
        "    print(\"Attempting to kill existing ngrok processes...\")\n",
        "    # Using pyngrok's kill method which is more reliable if pyngrok started them\n",
        "    ngrok.kill()\n",
        "    time.sleep(2) # Give it a moment to release resources\n",
        "    # A more forceful kill if ngrok.kill() doesn't catch everything\n",
        "    subprocess.run(['killall', '-q', 'ngrok'], check=False, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "    print(\"Previous ngrok processes (if any) should be stopped.\")\n",
        "except Exception as e:\n",
        "    print(f\"Note: Error during cleanup of ngrok (might be normal if none were running): {e}\")\n",
        "\n",
        "try:\n",
        "    print(\"Attempting to kill existing Streamlit processes (best effort)...\")\n",
        "    # This is a common way Streamlit is started from shell\n",
        "    # It's not foolproof for all ways Streamlit might be running\n",
        "    subprocess.run(\"pkill -f 'streamlit run dashboard.py'\", shell=True, check=False, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "    print(\"Previous Streamlit processes (if any) should be stopped.\")\n",
        "except Exception as e:\n",
        "    print(f\"Note: Error during cleanup of Streamlit (might be normal if none were running): {e}\")\n",
        "\n",
        "# --- Configure ngrok (Optional: if you have an authtoken and didn't run Cell 2) ---\n",
        "# If you have an ngrok authtoken and ran Cell 2, pyngrok should pick it up.\n",
        "# Alternatively, you can explicitly set it here if Cell 2 was skipped:\n",
        "# YOUR_NGROK_AUTHTOKEN = \"YOUR_ACTUAL_NGROK_AUTHTOKEN_HERE\"\n",
        "# if YOUR_NGROK_AUTHTOKEN != \"YOUR_ACTUAL_NGROK_AUTHTOKEN_HERE\":\n",
        "#     conf.get_default().auth_token = YOUR_NGROK_AUTHTOKEN\n",
        "#     print(\"Ngrok authtoken configured via pyngrok.conf.\")\n",
        "# else:\n",
        "#     print(\"Ngrok authtoken not explicitly set in this cell. Relying on prior !ngrok authtoken command or no auth.\")\n",
        "\n",
        "\n",
        "# --- Start ngrok tunnel ---\n",
        "public_url = None\n",
        "ngrok_tunnel = None # To keep track of the tunnel object for cleaner disconnect\n",
        "\n",
        "try:\n",
        "    print(\"Attempting to start ngrok tunnel for Streamlit on port 8501...\")\n",
        "    # pyngrok will download ngrok binary if not found\n",
        "    ngrok_tunnel = ngrok.connect(addr=8501, proto=\"http\", bind_tls=True) # Streamlit default port is 8501\n",
        "    public_url = ngrok_tunnel.public_url\n",
        "    print(f\"Ngrok tunnel established successfully!\")\n",
        "    print(f\"  Public URL (HTTPS): {public_url}\")\n",
        "    print(f\"  Local address being tunneled: {ngrok_tunnel.config['addr']}\")\n",
        "except Exception as e:\n",
        "    print(f\"ERROR: Ngrok connection failed: {e}\")\n",
        "    print(\"Attempting to check for existing tunnels...\")\n",
        "    try:\n",
        "        tunnels = ngrok.get_tunnels()\n",
        "        if tunnels:\n",
        "            print(\"Found existing ngrok tunnels:\")\n",
        "            for tunnel in tunnels:\n",
        "                print(f\"  - {tunnel.public_url} (proto: {tunnel.proto}) -> {tunnel.config['addr']}\")\n",
        "                # If a suitable tunnel already exists for port 8501, try to use it\n",
        "                if tunnel.proto in ['http', 'https'] and tunnel.config['addr'] in ['http://localhost:8501', 'localhost:8501', '127.0.0.1:8501', ':8501']:\n",
        "                    public_url = tunnel.public_url # Use the public URL of the existing tunnel\n",
        "                    ngrok_tunnel = tunnel # Keep track of this existing tunnel\n",
        "                    print(f\"Re-using existing tunnel: {public_url}\")\n",
        "                    break # Stop after finding a suitable one\n",
        "        else:\n",
        "            print(\"No existing ngrok tunnels found.\")\n",
        "    except Exception as e_tunnels:\n",
        "        print(f\"ERROR: Could not query existing ngrok tunnels: {e_tunnels}\")\n",
        "\n",
        "# --- Run Streamlit in the FOREGROUND ---\n",
        "if public_url:\n",
        "    print(f\"\\nTo access your Streamlit app, please open this URL in your browser: {public_url}\")\n",
        "    print(\"\\nStarting Streamlit application in the foreground...\")\n",
        "    print(\"Logs from Streamlit will appear below.\")\n",
        "    print(\"The Colab cell will keep running until you manually stop it (e.g., click the stop button next to the cell).\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    # Execute Streamlit. This will block until Streamlit is stopped.\n",
        "    # If dashboard.py has an error, it should print here.\n",
        "    try:\n",
        "        subprocess.run([\"streamlit\", \"run\", \"dashboard.py\"], check=True)\n",
        "    except FileNotFoundError:\n",
        "        print(\"ERROR: 'streamlit' command not found. Make sure Streamlit is installed correctly in the Colab environment.\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"ERROR: Streamlit process exited with an error (return code {e.returncode}).\")\n",
        "        if e.stdout:\n",
        "            print(\"Streamlit stdout:\\n\", e.stdout.decode())\n",
        "        if e.stderr:\n",
        "            print(\"Streamlit stderr:\\n\", e.stderr.decode())\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nStreamlit run interrupted by user (KeyboardInterrupt).\")\n",
        "    finally:\n",
        "        print(\"-\" * 70)\n",
        "        print(\"Streamlit process has finished or been stopped.\")\n",
        "else:\n",
        "    print(\"\\nStreamlit application will NOT be started because an ngrok tunnel could not be established.\")\n",
        "    print(\"Please check ngrok error messages above.\")\n",
        "\n",
        "# --- Disconnect ngrok when done (after Streamlit stops) ---\n",
        "if ngrok_tunnel: # If we successfully created or found a tunnel\n",
        "    print(\"\\nAttempting to disconnect ngrok tunnel...\")\n",
        "    try:\n",
        "        ngrok.disconnect(ngrok_tunnel.public_url) # Disconnect specific tunnel\n",
        "        print(f\"Ngrok tunnel {ngrok_tunnel.public_url} disconnected.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Error disconnecting ngrok tunnel {ngrok_tunnel.public_url if ngrok_tunnel else 'N/A'}: {e}\")\n",
        "        print(\"You might need to manually stop ngrok processes if issues persist or use ngrok.kill().\")\n",
        "\n",
        "# Final attempt to kill all ngrok processes to ensure cleanup\n",
        "try:\n",
        "    ngrok.kill()\n",
        "    print(\"All ngrok processes have been requested to stop.\")\n",
        "except Exception as e:\n",
        "    print(f\"Warning: Error during final ngrok.kill(): {e}\")\n",
        "\n",
        "print(\"\\nCell execution finished.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N86RDF7Laq4j",
        "outputId": "069a4c7b-aec8-498a-8c24-a2ae4a834918"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to kill existing ngrok processes...\n",
            "Previous ngrok processes (if any) should be stopped.\n",
            "Attempting to kill existing Streamlit processes (best effort)...\n",
            "Previous Streamlit processes (if any) should be stopped.\n",
            "Attempting to start ngrok tunnel for Streamlit on port 8501...\n",
            "Ngrok tunnel established successfully!\n",
            "  Public URL (HTTPS): https://3225-34-48-157-79.ngrok-free.app\n",
            "  Local address being tunneled: http://localhost:8501\n",
            "\n",
            "To access your Streamlit app, please open this URL in your browser: https://3225-34-48-157-79.ngrok-free.app\n",
            "\n",
            "Starting Streamlit application in the foreground...\n",
            "Logs from Streamlit will appear below.\n",
            "The Colab cell will keep running until you manually stop it (e.g., click the stop button next to the cell).\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Streamlit run interrupted by user (KeyboardInterrupt).\n",
            "----------------------------------------------------------------------\n",
            "Streamlit process has finished or been stopped.\n",
            "\n",
            "Attempting to disconnect ngrok tunnel...\n",
            "Ngrok tunnel https://3225-34-48-157-79.ngrok-free.app disconnected.\n",
            "All ngrok processes have been requested to stop.\n",
            "\n",
            "Cell execution finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9qIxIsZzUpjg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}